<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Conceptualizing and Measuring Algorithmic Fairness</title>
  </head>

  <body>
    <article id="cover">
      <div style="margin-left: 100px; margin-top: 30px; width: 750px">
        <h1>Conceptualizing and Measuring Algorithmic Fairness</h1>
        <br />
        <div style="font-size: 24px; color: white">
          <h2>Presented by:</h2>
          <h3 style="color: white">Mark A. Zaydman, MD, PhD</h3>
          <p>Assistant Professor of Pathology and Immunology</p>
        </div>
      </div>
    </article>

    <article id="Introduction">
      <section id="Obermeyer-IO">
        <h2>A case study of an unfair algorithm</h2>
        <br /><br /><br /><br /><br /><br /><br />
        <img src="{{assets_dir}}/Obermeyer_IO.png" alt="case-study-io" />
      </section>
      <section id="Obermeyer-Application">
        <h2>A case study of an unfair algorithm</h2>
        <p>
          This tool was broadly deployed, affecting ~200 million patients per
          year in USA
        </p>
        <br /><br />
        <img
          src="{{assets_dir}}/Obermeyer_Algorithm_Application.png"
          alt="case-study-io"
          style="width: 800px"
        />
      </section>

      <section id="obermeyer-story">
        <main>
          <div class="column" id="left-column">
            <h2>A case study of an unfair algorithm</h2>

            <br />
            <p>
              Black patient's had to be sicker than White patients to receive
              the same risk score and additional resources
            </p>
            <br /><br /><br /><br /><br />
            <div class="reference">
              Ziad Obermeyer et al. ,Dissecting racial bias in an algorithm used
              to manage the health of populations. Science 366,447-453(2019).
            </div>
          </div>
          <div class="column" id="right-column">
            <img
              src="{{assets_dir}}/obermeyer.jpeg"
              alt="obermeyer-result"
              style="width: 550px; margin-top: 0px; padding-top: 0px"
            />
          </div>
        </main>
      </section>

      <section id="outline">
        <h2>Content of this talk</h2>
        <main>
          <div class="column" id="left-column">
            <ol>
              <li>Define fairness concepts</li>
              <li>Apply fairness concepts</li>
              <li>Challenges to operationalizing fairness concepts</li>
            </ol>
          </div>
          <div class="column" id="right-column">
            <img src="{{assets_dir}}/road-map.jpg" alt="road-map" />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
    </article>

    <article id="Defining Fairness">
      <section id="defining-fairness">
        <h2>Defining fairness</h2>
        <main>
          <div class="column">
            <h4>General</h4>
            <p>
              A fair algorithm will treat people equally without bias or
              favoritism
            </p>
            <h4>Specific and measurable</h4>
            <ol>
              <li>Fairness through unawareness</li>
              <li>Individual fairness</li>
              <li>Demographic parity</li>
              <li>Equalized odds</li>
              <li>Predictive parity</li>
              <li>Equalized outcomes</li>
            </ol>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/fairness.jpeg"
              alt="fairness"
              style="width: 600px"
            />
          </div>
        </main>
      </section>

      <!-- <section id="need-to-measure">
        <h2>A formal definition of algorithmic fairness is needed</h2>
        <main>
          <div class="column" style="margin-left: 20px">
            <h4>Motivation</h4>
            <ul>
              <li>Benchmarking</li>
              <li>Accountability</li>
              <li>Standardization</li>
              <li>Quality assurance</li>
            </ul>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/need-to-measure.jpg"
              alt="need-to-measure"
            />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section> -->

      <section id="fairness-through-unawareness">
        <h2>Fairness through unawareness</h2>
        <main>
          <div class="column" id="column">
            <br />
            <p>
              An algorithm that is blinded to sensitive patient attributes will
              not exhibit biases based on those attributes
            </p>
            <br />
            <h4>Sensitive attributes</h4>
            <ul>
              <li>Race</li>
              <li>Gender</li>
              <li>Sexual orientation</li>
              <li>Religion</li>
              <li>Disability status</li>
            </ul>
          </div>
          <div class="column" id="column">
            <img src="{{assets_dir}}/head_in_sand.jpg" alt="head-in-sand" />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="fairness-through-unawareness-limitations">
        <h2>Fairness through unawareness</h2>
        <main>
          <div class="column" id="credit-column">
            <h4 style="text-decoration: underline">Limitation</h4>
            <p>Unawareness may not be achievable</p>
            <br /><br />
            <div class="reference">
              Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards
              Fair Machine Learning Algorithms in Laboratory Medicine, The
              Journal of Applied Laboratory Medicine, Volume 8, Issue 1, January
              2023, Pages 113–128
            </div>
          </div>
          <div class="column" id="content-column">
            <img
              src="{{assets_dir}}/feature-correlations.png"
              alt="feature-correlations"
              style="width: 900px"
            />
          </div>
        </main>
      </section>

      <section id="obermeyer-fairness-through-unawareness">
        <main>
          <div class="column">
            <h2>Fairness through unawareness</h2>
            <br />
            <p>
              The algorithm should be blinded to sensitive patient attributes
            </p>
            <br /><br /><br /><br /><br />
            <p style="font-style: italic">
              "...the algorithm takes in a large set of raw insurance claims
              data (features) ... demographics (e.g., age, sex), insurance type,
              diagnosis and procedure codes, medications, and detailed costs.
              <strong>
                Notably, the algorithm specifically excludes race. </strong
              >" (Obermeyer et al., page 3)
            </p>
            <br /><br /><br />
            <div style="border: solid; border-radius: 10px; text-align: center">
              <p>
                <strong>Problem:</strong> healthcare consumption per capita is
                significantly correlated with race
              </p>
            </div>
            <!-- </div> -->
          </div>
        </main>
      </section>

      <section id="individual-fairness">
        <h2>Individual fairness</h2>
        <main>
          <div class="column">
            <p>"Similar" individuals should be treated "similarly"</p>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/similar-people.jpg"
              alt="individual-fairness"
            />
            <div class="reference">Created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="individual-fairness-limitation">
        <h2>Individual fairness</h2>
        <main>
          <div class="column">
            <p>
              Proximal individuals in the input space should map with similar
              probability to the outcomes in the output space
            </p>
            <img
              src="{{assets_dir}}/individual-fairness.png"
              alt="individual-fairness"
            />
          </div>
        </main>
      </section>
      <section id="individual-fairness-limitation">
        <h2>Individual fairness</h2>
        <main>
          <div class="column">
            <p>
              Proximal individuals in the input space should map with similar
              probability to the outcomes in the output space
            </p>
            <img
              src="{{assets_dir}}/individual-fairness.png"
              alt="individual-fairness"
            />
            <br />
            <p
              style="
                text-align: center;
                border: solid;
                border-radius: 10px;
                border-color: #a51417;
              "
            >
              Limitation: the choice of features used to gauge individual
              similarity may introduce bias
            </p>
          </div>
        </main>
      </section>

      <section id="obermeyer-individual-fairness">
        <main>
          <div class="column">
            <img
              src="{{assets_dir}}/obermeyer_costs.png"
              alt="obermeyer-costs"
              style="width: 575px; margin-top: 0px; padding-top: 0px"
            />
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/obermeyer.png"
              alt="obermeyer-comorbidity"
              style="width: 560px; margin-top: 0px; padding-top: 0px"
            />
          </div>
        </main>
      </section>

      <section id="group-fairness">
        <h2>Group fairness</h2>
        <main>
          <div class="column">
            <p>Algorithms should treat different groups of people similarly</p>

            <br />
            <h4>Group fairness concepts</h4>
            <ul>
              <li>Demographic parity</li>
              <li>Equalized odds</li>
              <li>Predictive parity</li>
            </ul>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/group-fairness-gender.jpg"
              alt="group-fairness"
            />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="group-fairness-limitations">
        <h2>Group fairness</h2>
        <main>
          <div class="column">
            <h4>Problems with group labels</h4>
            <ul>
              <li>Societal constructs</li>
              <li>Nonrandom missingness and unreliability</li>
              <li>Group heterogeneity</li>
              <li>Intersectionality</li>
            </ul>
          </div>
          <div class="column">
            <img src="{{assets_dir}}/group-labels-2.jpg" alt="which-group" />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
      <section id="demographic-parity">
        <main>
          <div class="column">
            <h2>Demographic parity</h2>
            <br />
            <p>
              The algorithmic output should be independent of group membership
            </p>
            <br /><br />
            <img
              src="{{assets_dir}}/demographic_parity_scorecard.png"
              alt="demographic-parity-scorecard"
              style="width: 350px"
            />
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/demographic_parity_scenario.png"
              alt="demographic-parity-scenario"
              style="width: 450px"
            />
          </div>
        </main>
        <br />
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>
      <section id="obermeyer-demographic-parity">
        <h2>Demographic parity</h2>
        <br />
        <p>Algorithmic output should be independent of group membership</p>
        <br /><br />
        <h4>Black patients represented</h4>
        <main>
          <div class="column" id="left-column">
            <h4 style="font-size: 64px">12.3%</h4>
            <p>of total cohort</p>
          </div>
          <div class="column" id="right-column">
            <h4 style="font-size: 64px">17.7%</h4>
            <p>of those auto-enrolled based on algorithmic score</p>
          </div>
        </main>
        <br /><br />
        <p style="text-align: center">
          (Should have been
          <strong>46.5%</strong> if similarly sick patients received equal
          score)
        </p>
        <div class="ref">
          Ziad Obermeyer et al. ,Dissecting racial bias in an algorithm used to
          manage the health of populations. Science 366,447-453(2019).
        </div>
      </section>

      <section id="equalized-odds">
        <main>
          <div class="column">
            <h2>Equalized odds</h2>
            <br />
            <p>
              The algorithmic output should be independent of group membership
              when conditioned upon the true outcome
            </p>
            <br />
            <img
              src="{{assets_dir}}/equalized_odds_scorecard.png"
              alt="equalized-odds-scorecard"
              style="width: 200px"
            />
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/equalized_odds_scenario.png"
              alt="equalized-odds-scenario"
              style="width: 460px"
            />
          </div>
        </main>
        <br />
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>

      <!-- <section id="obermeyer-equalized-odds">
        <main>
          <div class="column" id="left-column">
            <h2>Equalized odds</h2>
            <br />
            <p>
              Algorithmic output should be independent of group membership when
              conditioned upon the true outcome
            </p>
            <br /><br /><br />
            <div class="reference">
              Ziad Obermeyer et al. ,Dissecting racial bias in an algorithm used
              to manage the health of populations. Science 366,447-453(2019).
            </div>
          </div>
          <div class="column" id="right-column">
            <img
              src="{{assets_dir}}/obermeyer.png"
              alt="obermeyer-equalized-odd"
              style="width: 540px; margin-top: 0px; padding-top: 0px"
            />
          </div>
        </main>
      </section> -->

      <section id="obermeyer-equalized-odds">
        <main>
          <div class="column" id="left-column">
            <h2>Equalized odds</h2>
            <br />
            <p>
              Algorithmic output should be independent of group membership when
              conditioned upon the true outcome
            </p>
            <br /><br /><br />
            <div class="reference">
              Ziad Obermeyer et al. ,Dissecting racial bias in an algorithm used
              to manage the health of populations. Science 366,447-453(2019).
            </div>
          </div>
          <div class="column" id="right-column">
            <img
              src="{{assets_dir}}/obermeyer-equalized-odds.png"
              alt="obermeyer-equalized-odd"
              style="width: 600px; margin-top: 0px; padding-top: 0px"
            />
          </div>
        </main>
      </section>

      <section id="predictive-parity">
        <main>
          <div class="column">
            <h2>Predictive parity</h2>
            <br />
            <p>
              The probability of the true outcome should be independent of group
              membership when conditioned upon the algorithmic output
            </p>
            <br /><br />
            <img
              src="{{assets_dir}}/pred_parity_scorecard.png"
              alt="predictive-parity-scorecard"
              style="width: 250px"
            />
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/pred_parity_scenario.png"
              alt="predictive-parity-scenario"
              style="width: 450px"
            />
          </div>
        </main>
        <br />
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>

      <section id="obermeyer-predictive-parity">
        <main>
          <div class="column" id="left-column">
            <h2>Predictive parity</h2>
            <br />
            <p>
              Probability of the true outcome should be independent of group
              membership when conditioned upon the algorithmic output
            </p>
            <br /><br /><br />
            <div class="reference">
              Ziad Obermeyer et al. ,Dissecting racial bias in an algorithm used
              to manage the health of populations. Science 366,447-453(2019).
            </div>
          </div>
          <div class="column" id="right-column">
            <img
              src="{{assets_dir}}/obermeyer-predictive-parity.jpeg"
              alt="obermeyer-result"
              style="width: 550px; margin-top: 0px; padding-top: 0px"
            />
          </div>
        </main>
      </section>

      <section id="group-concept-exclusivity">
        <h2>Exclusivity of group fairness concepts</h2>
        <main>
          <div class="column">
            <img
              src="{{assets_dir}}/group_fairness.png"
              alt="group-fairness-exclusivity"
            />
          </div>
        </main>
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>

      <section id="group-concept-exclusivity">
        <h2>Exclusivity of group fairness concepts</h2>
        <br /><br />
        <main>
          <div class="column">
            <p>
              It is not possible to satisfy all group fairness concepts unless:
            </p>
            <ol>
              <li>The algorithm lacks any predictive skill</li>
              <li>The base rate of the disease is the same across groups</li>
            </ol>
            <br /><br /><br />
          </div>
        </main>
      </section>

      <section id="group-concept-exclusivity">
        <h2>Exclusivity of group fairness concepts</h2>
        <br /><br />
        <main>
          <div class="column">
            <p>
              It is not possible to satisfy all group fairness concepts unless:
            </p>
            <ol>
              <li>The algorithm lacks any predictive skill</li>
              <li>The base rate of the disease is the same across groups</li>
            </ol>
            <br /><br /><br />
            <p
              style="
                text-align: center;
                border: solid;
                border-radius: 10px;
                font-size: xx-large;
                font-weight: bold;
              "
            >
              It is not possible to have a perfectly fair algorithm in an unfair
              world
            </p>
          </div>
        </main>
      </section>

      <section id="lessons-of-exclusivity">
        <h2>What to do?</h2>
        <br /><br />
        <p
          style="
            text-align: center;
            border: solid;
            border-radius: 10px;
            font-size: xx-large;
            font-weight: bold;
          "
        >
          It is not possible to have a perfectly fair algorithm in an unfair
          world
        </p>
        <main>
          <div class="column">
            <ol>
              <li>
                Advocate for our patients inside and outside the confines of
                medicine
              </li>
              <li>
                Stop focusing on intermediate statistical notions of fairness
              </li>
            </ol>
          </div>
        </main>
      </section>

      <section id="equity">
        <h2>Equalized outcomes (equity)</h2>
        <main>
          <div class="column">
            <p>
              The notion that fairness occurs when all individuals get the same
              opportunity to enjoy good health
            </p>
            <br />
            <img
              src="{{assets_dir}}/equity.png"
              alt="equity"
              style="width: 550px"
            />
          </div>
        </main>
        <div class="reference">
          https://interactioninstitute.org/illustrating-equality-vs-equity/
        </div>
      </section>

      <section id="obermeyer-equalized-outcomes">
        <main>
          <div class="column" id="left-column">
            <h2>Equalized outcomes</h2>
            <br />
            <p>How will this algorithm affect health equity?</p>
            <br /><br /><br />
            <div class="referenc">
              Ziad Obermeyer et al. ,Dissecting racial bias in an algorithm used
              to manage the health of populations. Science 366,447-453(2019).
            </div>
          </div>
          <div class="column" id="right-column">
            <img
              src="{{assets_dir}}/obermeyer.png"
              alt="obermeyer-result"
              style="width: 500px; margin-top: 0px; padding-top: 0px"
            />
          </div>
        </main>
      </section>
    </article>

    <article id="moving forward">
      <section id="building-fairness-infrastructure">
        <h2>
          What is needed to operationalize fairness metrics as a quality domain?
        </h2>
        <main>
          <div class="column">
            <ul>
              <li>Training and education</li>
              <li>Data and analytics</li>
              <li>Standards and processes</li>
              <li>Benchmarks and accountability</li>
            </ul>
          </div>
          <div class="column">
            <img
              src="{{assets_dir}}/call-to-action.jpg"
              alt="obermeyer-result"
              style="width: 500px; margin-top: 0px; padding-top: 0px"
            />
            <div class="reference">Image created by Dall-E</div>
          </div>
        </main>
      </section>
    </article>

    <article id="summary">
      <section id="concept-summary">
        <h2>Key take away points</h2>
        <main>
          <div class="column">
            <ol>
              <li>There are many definitions of fairness</li>
              <li>Different definitions may be in conflict</li>
              <li>
                It is best to prioritize equity over intermediate statistical
                notions
              </li>
              <li>
                There are many challenges to translating fairness metrics into
                current laboratory practice
              </li>
            </ol>
          </div>
        </main>
      </section>
    </article>

    <article id="aknowledgements">
      <section id="aknowledgements-slide">
        <h2>Aknowledgements</h2>
        <main>
          <div class="column" id="column">
            <img
              src="{{assets_dir}}/azimi.jpeg"
              alt="vahid-azimi"
              style="width: 300px; margin-left: 40px"
            />
          </div>
          <div class="column" style="margin-left: 0px; padding-left: 0px">
            <h4>Vahid Azimi, MD</h4>
            <p style="font-size: 18px">
              Assistant Professor<br />
              Department of Pathology and Immunology<br />
              Washington University School of Medicine
            </p>
          </div>
          <div class="column" id="column">
            <img
              src="{{assets_dir}}/jalmcover.png"
              alt="jalm-cover"
              style="width: 320px"
            />
          </div>
        </main>
        <div class="reference">
          Vahid Azimi, Mark A Zaydman, Optimizing Equity: Working towards Fair
          Machine Learning Algorithms in Laboratory Medicine, The Journal of
          Applied Laboratory Medicine, Volume 8, Issue 1, January 2023, Pages
          113–128
        </div>
      </section>
    </article>
  </body>
</html>
